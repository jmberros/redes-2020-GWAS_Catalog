{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos y limpieza inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(data_dir + 'GwasCat_associations.r2020-05-03.parsed.csv.gz')\n",
    "data = pd.read_table(data_dir + 'gwas_cat.filtrado.tsv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fenotipo.unique()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos aquellas instancias que tienen en la columna `alelo_riesgo` un SNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_snps = data.alelo_riesgo.str.match(\"^rs[0-9]+-[ATCG\\?]$\")\n",
    "data = data[mask_snps]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miramos y descartamos valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No nos interesan aquellas instancias que tienen valore faltantes en la columna `OR_or_beta`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = ['OR_or_beta'], inplace = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESTO SE PUEDE MEJORAR: tiene que haber una única componente gigante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opcional 1**: sacamos aquellos alelos que aparezcan una sola vez. Sirve más que nada para achicar la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    umbral = 1\n",
    "    mask_alelos_poca_frecuencia = data.alelo_riesgo.value_counts() <=1\n",
    "    mask_alelos_poca_frecuencia.head()\n",
    "\n",
    "    alelos_poca_frecuencia = list(mask_alelos_poca_frecuencia.index[mask_alelos_poca_frecuencia.values])\n",
    "    print(len(alelos_poca_frecuencia))\n",
    "\n",
    "    mask_descartables = data.alelo_riesgo.isin(alelos_poca_frecuencia)\n",
    "    mask_descartables\n",
    "\n",
    "    print(data.shape)\n",
    "    data = data[~mask_descartables]\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opcional 2:** Sacamos aquellos fenotipos que aparecen una sola vez:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umbral = 1\n",
    "mask_fenotipos_poca_frecuencia = data.fenotipo.value_counts() <=1\n",
    "mask_fenotipos_poca_frecuencia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenotipos_poca_frecuencia = list(mask_fenotipos_poca_frecuencia.index[mask_fenotipos_poca_frecuencia.values])\n",
    "print(len(fenotipos_poca_frecuencia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_descartables = data.fenotipo.isin(fenotipos_poca_frecuencia)\n",
    "mask_descartables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "data = data[~mask_descartables]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tripletes `alelo_riesgo`, `fenotipo` y `OR_or_beta` - Matriz de utilidad/¿incidencia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tripletes = data[['alelo_riesgo', 'fenotipo', 'OR_or_beta']]\n",
    "data_tripletes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tripletes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matriz de Utilidad/¿incidencia?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alelos = data_tripletes.alelo_riesgo.nunique()\n",
    "n_fenotipos = data_tripletes.fenotipo.nunique()\n",
    "\n",
    "data_bipartita = pd.DataFrame(np.zeros((n_alelos, n_fenotipos)), columns = data_tripletes.fenotipo.value_counts().index, dtype = np.int8)\n",
    "data_bipartita.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EN EL DICCIONARIO APARECEN ORDENADOS POR FRECUENCIA\n",
    "dict_id_to_alelo = {}\n",
    "for i,alelo in enumerate(data_tripletes.alelo_riesgo.value_counts().index):\n",
    "    dict_id_to_alelo[i] = alelo\n",
    "    \n",
    "dict_alelo_to_id = {v: k for k, v in dict_id_to_alelo.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _, row in data_tripletes.iterrows():\n",
    "    alelo_row = row.alelo_riesgo\n",
    "    fenotipo_row = row.fenotipo\n",
    "    id_alelo = dict_alelo_to_id[alelo_row]\n",
    "    data_bipartita.loc[id_alelo, fenotipo_row] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chequeos**\n",
    "\n",
    "La suma de todos los elementos de `data_bipartita` debe dar la cantidad de filas en `data_tripletes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita.sum().sum() == data_tripletes.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suma por columnas el `value_counts()` de `data_tripletes.fenotipo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_tripletes.fenotipo.value_counts().values == data_bipartita.sum().values).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La suma por filas el `value_counts()` de `data_tripletes.alelo_riesgo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_tripletes.alelo_riesgo.value_counts().values == data_bipartita.sum(axis = 1).values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(data_bipartita.sum().values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separación de datos para testeo\n",
    "\n",
    "#### Separación 1 - Alelos \"nuevos\"\n",
    "\n",
    "Tomamos alelos al azar y los sacamos de la matriz de utilidad. De esta forma, no aportarán a las similitudes de los fenotipos.\n",
    "\n",
    "Vamos a sacar alelos que tengan al menos grado 2 y como máximo grados 6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_alelos_a_sacar = 100\n",
    "alelos_a_sacar = []\n",
    "\n",
    "lista_alelos_desordenada = list(dict_id_to_alelo.keys())\n",
    "lista_alelos_desordenada = np.random.choice(lista_alelos_desordenada,len(lista_alelos_desordenada), replace = False)\n",
    "\n",
    "grados = data_bipartita.sum(axis = 1)\n",
    "for alelo in lista_alelos_desordenada:\n",
    "#     pass\n",
    "    grado = grados.iloc[alelo]\n",
    "    \n",
    "    if grado >=3 and grado <=5:\n",
    "        alelos_a_sacar.append(alelo)\n",
    "        \n",
    "    if len(alelos_a_sacar) == n_alelos_a_sacar:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita_test_1 = data_bipartita.iloc[alelos_a_sacar,:]\n",
    "print(data_bipartita_test_1.shape)\n",
    "print(data_bipartita_test_1.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita_train = data_bipartita.drop(alelos_a_sacar)\n",
    "print(data_bipartita_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESTO SE PUEDE MEJORAR, PERO POR AHORA CHEQUEAMOS QUE NO QUEDE UN FENOTIPO DE GRADO 1\n",
    "Nuevamente, lo mejor sería que el conjunto de train tenga una única componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita_train.sum().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacamos del conjunto de test algunos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "for idx, row in data_bipartita_test_1.iterrows():\n",
    "    \n",
    "    fenotipos_asociados = list(row[row == 1].index)\n",
    "    fenotipos_asociados = np.random.choice(fenotipos_asociados, len(fenotipos_asociados), replace = False)\n",
    "    contador = 0\n",
    "    for fenotipo in fenotipos_asociados:\n",
    "        data_bipartita_test_1.loc[idx, fenotipo] = 0\n",
    "        contador +=1\n",
    "        if contador == len(fenotipos_asociados) - 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita_test_1.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Armamos la red --> ¿Pasar a Gephy la visualización?\n",
    "\n",
    "Se puede saltear por ahora, no lo usamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red.add_edges_from(data_tripletes[['alelo_riesgo', 'fenotipo']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MUY PESADO EN MEMORIA\n",
    "matriz_adyacencia = nx.to_pandas_adjacency(red, dtype = np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba = np.ones((99365, 99365), dtype = np.uint8)\n",
    "# del prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matriz_adyacencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Filtro Colaborativo Implícito\n",
    "\n",
    "https://medium.com/radon-dev/item-item-collaborative-filtering-with-binary-or-unary-data-e8f0b465b2c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, normalizamos con respecto a la cantidad de enlaces que tiene cada alelo. En las películas, esto se justifica como \n",
    "\n",
    "> *This is the idea of normalizing the user vectors so that a user with many ratings contributes less to any individual rating. This is to say that a like from a user who has only liked 10 items is more valuable to us than a like from someone who likes everything she comes across.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------\n",
    "# ITEM-ITEM CALCULATIONS\n",
    "#------------------------\n",
    "\n",
    "# As a first step we normalize the user vectors to unit vectors.\n",
    "\n",
    "# magnitude = sqrt(x2 + y2 + z2 + ...)\n",
    "magnitude = np.sqrt(np.square(data_bipartita_train).sum(axis=1))\n",
    "\n",
    "# # unitvector = (x / magnitude, y / magnitude, z / magnitude, ...)\n",
    "data_bipartita_normalizada = data_bipartita_train.divide(magnitude, axis='index')\n",
    "data_bipartita_normalizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego,calculamos la similaridad coseno para cada columna, obteniendo así qué fenotipos son parecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(data_items):\n",
    "    \"\"\"Calculate the column-wise cosine similarity for a sparse\n",
    "    matrix. Return a new dataframe matrix with similarities.\n",
    "    \"\"\"\n",
    "    data_sparse = sparse.csr_matrix(data_items)\n",
    "    similarities = cosine_similarity(data_sparse.transpose())\n",
    "    sim = pd.DataFrame(data=similarities, index= data_items.columns, columns= data_items.columns)\n",
    "    return sim\n",
    "\n",
    "# Build the similarity matrix\n",
    "data_matrix = calculate_similarity(data_bipartita_normalizada)\n",
    "data_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos similaridades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get the top 11 similar artists for Beyonce\n",
    "# print(data_matrix.loc['Drinking behavior'].nlargest(25))\n",
    "\n",
    "print(data_matrix.loc['Educational attainment (MTAG)'].nlargest(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a new dataframe with the 10 closest neighbours (most similar)\n",
    "# for each artist.\n",
    "\n",
    "n_neighbours = 20\n",
    "data_neighbours = pd.DataFrame(index=data_matrix.columns, columns=range(1,n_neighbours+1))\n",
    "for i in range(0, len(data_matrix.columns)):\n",
    "    data_neighbours.iloc[i,:n_neighbours] = data_matrix.iloc[0:,i].sort_values(ascending=False)[:n_neighbours].index\n",
    "data_neighbours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(43)\n",
    "alelo_prueba = np.random.choice(data_tripletes.alelo_riesgo.unique())\n",
    "alelo_prueba = 'rs6739779-C'\n",
    "alelo_prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alelo_prueba_index = dict_alelo_to_id[alelo_prueba]\n",
    "print(alelo_prueba_index)\n",
    "\n",
    "known_alelo_fenotipos = data_bipartita_normalizada.iloc[alelo_prueba_index]\n",
    "known_alelo_fenotipos = known_alelo_fenotipos[known_alelo_fenotipos >0].index.values\n",
    "print(known_alelo_fenotipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the neighbourhood from the most similar items to the\n",
    "# ones our alelo it's related\n",
    "most_similar_fenotipos = data_neighbours.loc[known_alelo_fenotipos]\n",
    "most_similar_fenotipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_list = most_similar_fenotipos.values.tolist()\n",
    "similar_list = list(set([item for sublist in similar_list for item in sublist]))\n",
    "similar_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood = data_matrix[similar_list].loc[similar_list]\n",
    "neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user vector containing only the neighbourhood items and\n",
    "# the known user likes.\n",
    "fenotipos_probables = data_bipartita_normalizada.iloc[alelo_prueba_index].loc[similar_list]\n",
    "fenotipos_probables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score.\n",
    "score = neighbourhood.dot(fenotipos_probables).div(neighbourhood.sum(axis=1))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the known likes.\n",
    "# score = score.drop(known_alelo_fenotipos).sort_values()\n",
    "score.sort_values(ascending = False, inplace = True)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_alelo_fenotipos)\n",
    "print(score.nlargest(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobre Conjunto de Test\n",
    "\n",
    "\"A mano\" por ahora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(50)\n",
    "alelo_prueba_index = np.random.choice(data_bipartita_test_1.index)\n",
    "alelo_prueba_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alelo_prueba = dict_id_to_alelo[alelo_prueba_index]\n",
    "alelo_prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_alelo_fenotipos = data_bipartita_test_1.loc[alelo_prueba_index]\n",
    "known_alelo_fenotipos = known_alelo_fenotipos[known_alelo_fenotipos >0].index.values\n",
    "print(known_alelo_fenotipos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenotipos_reportados_alelo = data_bipartita.loc[alelo_prueba_index]\n",
    "fenotipos_reportados_alelo = fenotipos_reportados_alelo[fenotipos_reportados_alelo >0].index.values\n",
    "print(fenotipos_reportados_alelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the neighbourhood from the most similar items to the\n",
    "# ones our alelo it's related\n",
    "most_similar_fenotipos = data_neighbours.loc[known_alelo_fenotipos]\n",
    "most_similar_fenotipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_list = most_similar_fenotipos.values.tolist()\n",
    "similar_list = list(set([item for sublist in similar_list for item in sublist]))\n",
    "similar_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbourhood = data_matrix[similar_list].loc[similar_list]\n",
    "neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bipartita_test_1.loc[alelo_prueba_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A user vector containing only the neighbourhood items and\n",
    "# the known user likes.\n",
    "fenotipos_probables = data_bipartita_test_1.loc[alelo_prueba_index].loc[similar_list]\n",
    "fenotipos_probables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the score.\n",
    "score = neighbourhood.dot(fenotipos_probables).div(neighbourhood.sum(axis=1))\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the known likes.\n",
    "# score = score.drop(known_alelo_fenotipos).sort_values()\n",
    "score.sort_values(ascending = False, inplace = True)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(known_alelo_fenotipos)\n",
    "print(fenotipos_reportados_alelo)\n",
    "print(score.nlargest(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grafico de fenotipos vs score\n",
    "capacidad de priorizar dado \n",
    "Usar el paper de zhou\n",
    "\n",
    "\n",
    "MARTES - 5 min\n",
    "dimensión de los datos\n",
    "encuadrar el problema biológico o metodológico\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosas para hacer\n",
    "\n",
    "1. Chequear sobre `categoria_fenotipo` que no sea trivial\n",
    "1. Agregar segunda forma de evaluación: borrando '1' de la matriz de train\n",
    "1. Chequear que al separar train y test no se rompa la red\n",
    "1. Metodizar la evaluación para obtener una métrica\n",
    "1. Chequear que no estemos cayendo en la parte \"fácil\" de la red --> ¿pregunta biológica?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
